

YouTube’s parent company Google also announced it had disabled 210 channels on Friday for “coordinated influence operations” related to the protests, though it did not explicitly say that state interference had been at work.

Social media companies have been under intense pressure to shut down the state actors and suss out fake accounts, but analysts said the use of extreme language in politically motivated posts is a grey area given their reticence about policing political speech.

“They don’t want to take on the role of censor or big brother,” said Katharine Gelber, head of the school of political science and international studies at the University of Queensland.

SUBSCRIBE TO US China Trade War

SCMP NEWS. DELIVERED.

Sign up for our China Breaking News newsletter here

She said these companies must “walk that line” between “facilitating free speech and not acting as censors of political viewpoints they might or might not agree with” while limiting content that could “fuel harmful behaviour”.

On the platforms, such decisions about which posts do or do not violate policy are often subjective decisions and the two platforms apply different standards.

Both Facebook and Twitter’s publicly available definitions of hate speech do not include specific protections for people based on their political affiliation or occupation, but posts attacking people on the basis of race, ethnicity, national origin, religious affiliation, sexual orientation, gender, gender identity are generally grounds for a ban.

Direct incitement and threats of violence are also banned.

In Hong Kong, where many residents’ social media feeds have been overtaken by political posts and polarised commentary, some posts also include angry, derogatory speech and dehumanising messages, with police referred to as “dogs” and protesters as “cockroaches”.

Google, YouTube’s parent company, has been removing content from the platform. Photo: AP

Google, YouTube’s parent company, has been removing content from the platform. Photo: AP

 Such postings which may target protesters on the basis of their beliefs underline the “complexity” of social media monitoring, according to Gelber.

“It’s not overtly such an incitement to violence that it would pass an incitement to violence test, but it’s also not traditional hate speech, because its directed at pro-democracy protesters who are of the same ethnicity as the pro-Chinese [side],” she said.

“Any time you start getting language that dehumanises an identifiable subgroup of people by treating them as insects or vermin, implying they need to be eradicated, that is potentially very dangerous speech,” she said.

Social media companies are constantly updating policies and developing measures to address how languages, images, and information are across their platforms.

This is in addition to investigations surrounding coordinated influence campaigns like those banned this week.

Twitter, Facebook and YouTube have taken steps to police fake and inflammatory posts by altering algorithms, adding to the manpower dedicated to moderating and removing content.

Facebook took action against four million pieces of content containing hate speech in the first quarter of 2019, an increase of 700,000 on the previous quarter.

Twitter upped its response times to user reports of abusive accounts, suspending three times as many accounts within 24 hours of them being reported compared to the previous year, according to April’s figures.

Google took down over 75 million YouTube videos in the first three months of this year that were on channels violating their community standards.

Following the Christchurch shootings in March, all three companies signed agreements with world leaders to moderate extreme speech.

The companies are under constant pressure from governments and media to proactively police their platforms and react to the political impact that they have around the world, according to Andre Oboler, director of the Online Hate Prevention Institute in Sydney and a senior lecturer at the La Trobe law school.

“Where the platform could be seen as facilitating something like human rights abuses, that is a major risk factor and threat to the platform, so they need to resolve that issue, [especially] when it’s something that’s all over the mainstream media.”

Identifying and acting against misinformation campaigns like those that targeted the protests in Hong Kong is an important part of that work, as “hateful ideology” is increasingly being used as a “tool” in political “warfare”, Oboler, said, noting that the event follows on the heels of a Russian misinformation campaign targeting the 2016 US elections.

“The Hong Kong situation is an example of what could happen in relation to other countries … if there is no response to what China is doing now, and we have Russia before it, what could be next, what might other states do in terms of influencing another country?”
